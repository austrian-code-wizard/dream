  <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
  <html>

  <!-- ======================================================================= -->
  <script src="http://www.google.com/jsapi" type="text/javascript"></script>
  <script type="text/javascript">google.load("jquery", "1.3.2");</script>
  <style type="text/css">
    body {
      font-family: "Crimson Text","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
      font-weight:300;
      font-size:18px;
      margin-left: auto;
      margin-right: auto;
      width: 100%;
    }

    pre {
      background-color: #f6f8fa;
      padding: 16px;
    }

    code {
      font-family: "SFMono-Regular","Consolas","Liberation Mono","Menlo",monospace;
      overflow: scroll;
    }

    h1 {
      font-family: "Source Sans Pro";
      font-weight:300;
    }

    div {
      max-width: 95%;
      margin:auto;
      padding: 10px;
    }

    .table-like {
      display: flex;
      flex-wrap: wrap;
      flex-flow: row wrap;
      justify-content: center;
    }

    .disclaimerbox {
      background-color: #eee;
      border: 1px solid #eeeeee;
      border-radius: 10px ;
      -moz-border-radius: 10px ;
      -webkit-border-radius: 10px ;
      padding: 20px;
    }

    video.header-vid {
      height: 140px;
      border: 1px solid black;
      border-radius: 10px ;
      -moz-border-radius: 10px ;
      -webkit-border-radius: 10px ;
    }

    img {
      padding: 0;
      display: block;
      margin: 0 auto;
      max-height: 100%;
      max-width: 100%;
    }

    iframe {
      max-width: 100%;
    }

    img.header-img {
      height: 140px;
      border: 1px solid black;
      border-radius: 10px ;
      -moz-border-radius: 10px ;
      -webkit-border-radius: 10px ;
    }

    img.rounded {
      border: 1px solid #eeeeee;
      border-radius: 10px ;
      -moz-border-radius: 10px ;
      -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
      color: #1367a7;
      text-decoration: none;
    }
    a:hover {
      color: #208799;
    }

    td.dl-link {
      height: 160px;
      text-align: center;
      font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
              0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
              5px 5px 0 0px #fff, /* The second layer */
              5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
              10px 10px 0 0px #fff, /* The third layer */
              10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
              15px 15px 0 0px #fff, /* The fourth layer */
              15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
              20px 20px 0 0px #fff, /* The fifth layer */
              20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
              25px 25px 0 0px #fff, /* The fifth layer */
              25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
      margin-left: 10px;
      margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
              0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
              5px 5px 0 0px #fff, /* The second layer */
              5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
              10px 10px 0 0px #fff, /* The third layer */
              10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
      margin-top: 5px;
      margin-left: 10px;
      margin-right: 30px;
      margin-bottom: 5px;
    }

    .vert-cent {
      position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
      border: 0;
      height: 1px;
      max-width: 1100px;
      background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }

    #authors td {
      padding-bottom:5px;
      padding-top:30px;
    }
  </style>
  <!-- ======================================================================= -->

  <!-- Start : Google Analytics Code -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-64069893-4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-64069893-4');
  </script> -->
  <!-- End : Google Analytics Code -->

  <script type="text/javascript" src="resources/hidebib.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>
  <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">

  <head>
  <div max-width=100%>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="icon" href="favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="57x57" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon" sizes="60x60" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="76x76" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon" sizes="120x120" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon" sizes="144x144" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-152x152.png" />
    <link rel="apple-touch-icon" sizes="180x180" href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-180x180.png" />

    <link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-196x196.png" sizes="196x196" />
    <link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-192x192.png" sizes="192x192" />
    <link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-128.png" sizes="128x128" />
    <link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-96x96.png" sizes="96x96" />
    <link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-16x16.png" sizes="16x16" />

    <link rel="mask-icon" href="//www-media.stanford.edu/assets/favicon/safari-pinned-tab.svg" color="#ffffff">
    <meta name="application-name" content="Stanford University"/>
    <meta name="msapplication-TileColor" content="#FFFFFF" />
    <meta name="msapplication-TileImage" content="//www-media.stanford.edu/assets/favicon/mstile-144x144.png" />
    <meta name="msapplication-square70x70logo" content="//www-media.stanford.edu/assets/favicon/mstile-70x70.png" />
    <meta name="msapplication-square150x150logo" content="//www-media.stanford.edu/assets/favicon/mstile-150x150.png" />
    <meta name="msapplication-square310x310logo" content="//www-media.stanford.edu/assets/favicon/mstile-310x310.png" />
    <title>Decoupling Exploration and Exploitation in Meta-Reinforcement Learning without Sacrifices</title>
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="canonical" href="https://ezliu.github.io/" />
  <meta name="referrer" content="no-referrer-when-downgrade" />

  <meta property="og:site_name" content="Exploration in Meta-Reinforcement Learning" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="Decoupling Exploration and Exploitation in Meta-Reinforcement Learning without Sacrifices" />
  <meta property="og:description" content="Evan Z. Liu, Aditi Raghunathan, Percy Liang, Chelsea Finn. Decoupling Exploration and Exploitation in Meta-Reinforcement Learning without Sacrifices. 2020." />
  <meta property="og:url" content="https://ezliu.github.io/" />
  <meta property="og:image" content="https://ezliu.github.io/resources/blue.svg" />

  <meta property="article:publisher" content="https://ezliu.github.io/" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Decoupling Exploration and Exploitation in Meta-Reinforcement Learning without Sacrifices" />
  <meta name="twitter:description" content="Evan Z. Liu, Aditi Raghunathan, Percy Liang, Chelsea Finn. Decoupling Exploration and Exploitation in Meta-Reinforcement Learning without Sacrifices. 2020." />
  <meta name="twitter:url" content="https://ezliu.github.io/" />
  <meta name="twitter:image" content="https://ezliu.github.io/resources/red.png" />
  <meta property="og:image:width" content="1600" />
  <meta property="og:image:height" content="900" />

  <script src="https://www.youtube.com/iframe_api"></script>
  <meta name="twitter:player" content="https://www.youtube.com/embed/EiIC0Rkz8-s" />
  <meta name="twitter:player:width" content="640" />
  <meta name="twitter:player:height" content="360" />
</head>

<body>

      <br>
      <center><span style="font-size:44px;font-weight:bold;font-family:Source Sans Pro;">Decoupling Exploration and Exploitation in<br/>
           Meta-Reinforcement Learning without Sacrifices</span></center><br/>
      <div class="table-like" style="justify-content:space-evenly;max-width:900px;margin:auto;">
          <div><center><span style="font-size:25px"><a href="https://cs.stanford.edu/~evanliu" target="_blank">Evan Z. Liu</a></span></center>
          <center><span style="font-size:25px">Stanford</span></center>
          </div>

          <div><center><span style="font-size:25px"><a href="https://stanford.edu/~aditir/" target="_blank">Aditi Raghunathan</a></span></center>
          <center><span style="font-size:25px">Stanford</span></center>
          </div>

          <div><center><span style="font-size:25px"><a href="https://cs.stanford.edu/~pliang/" target="_blank">Percy Liang</a></span></center>
          <center><span style="font-size:25px">Stanford</span></center>
          </div>

          <div><center><span style="font-size:25px"><a href="https://ai.stanford.edu/~cbfinn/" target="_blank">Chelsea Finn</a></span></center>
          <center><span style="font-size:25px">Stanford</span></center>
          </div>
          <center><span style="font-size:25px">International Conference on Machine Learning (ICML), 2021</span></center>
      </div>

      <div class="table-like" style="justify-content:space-evenly;max-width:700px;margin:auto;padding:5px">
        <div><center><span style="font-size:28px"><a href="https://arxiv.org/abs/2008.02790">[Paper]</a></span></center></div>
        <div><center><span style="font-size:28px"><a href="https://ai.stanford.edu/blog/meta-exploration/">[Blog Post]</a></span></center></div>
        <div><center><span style="font-size:28px"><a href='https://github.com/ezliu/dream'>[Code]</a></span></center> </div>
      </div>

      <center>
      <iframe width="768" height="432" src="https://www.youtube.com/embed/EiIC0Rkz8-s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      <br>

      <div style="width:800px; margin:0 auto" align="justify">
        <p><b>Abstract.</b> The goal of meta-reinforcement learning (meta-RL)
        is to build agents that can quickly learn new tasks by leveraging prior
        experience on related tasks.  Learning a new task often requires both
        exploring to gather task-relevant information and exploiting this
        information to solve the task. In principle, optimal exploration and
        exploitation can be learned end-to-end by simply maximizing task
        performance. However, such meta-RL approaches struggle with local
        optima due to a chicken-and-egg problem: learning to explore requires
        good exploitation to gauge the exploration's utility, but learning to
        exploit requires information gathered via exploration. Optimizing
        separate objectives for exploration and exploitation can avoid this
        problem, but prior meta-RL exploration objectives yield suboptimal
        policies that gather information irrelevant to the task. We alleviate
        both concerns by constructing an exploitation objective that
        automatically identifies task-relevant information and an exploration
        objective to recover only this information. This avoids local optima in
        end-to-end training, without sacrificing optimal exploration.
        Empirically, DREAM substantially outperforms existing approaches on
        complex meta-RL problems, such as sparse-reward 3D visual navigation.
      </p>
      </div>
      <br><hr>

      <div style="width:800px; margin:0 auto; text-align=left", align="left">
      <center id="videos"><h1>Qualitative Analysis of DREAM</h1></center>
        <p>We provide visualizations and analysis of the policies learned by DREAM and other approaches below.</p>

        <center><h2>Distracting Bus</h2></center>

        <center>
          <a href="resources/city_examples.svg"><img src="resources/city_examples.svg" width="800px"></img></a>
          <p align="center" style="width:600px">
            Examples of different problems and goals.
            In different <i>problems</i>, the colored buses in the corners
            change positions to be one of the 4! permutations.
            In different <i>episodes</i>, the goal location changes to one of the four corners.
          </p>
        </center>

        <p>
          In the <i>distracting bus</i> task, the agent must go to the goal
          location (green square), which is randomly placed in one of the four
          corners, in as few timesteps as possible.
          In different problems, the destinations of the buses is randomized to
          different permutations.
          See the two examples above.
        </p>

        <div class="table-like" style="justify-content:space-evenly;max-width:1100px;margin:auto;padding:0px">
          <div>
            <a href="resources/city/dream_exploration.gif"><img src="resources/city/dream_exploration.gif" width="375px" style="padding:0px"></img></a>
            <center><p align="center"><b>Exploration</b> learned by DREAM.</p></center>
          </div>

          <div>
            <a href="resources/city/dream_exploitation.gif"><img src="resources/city/dream_exploitation.gif" width="375px" style="padding:0px"></img></a>
            <center><p align="center"><b>Exploitation</b> learned by DREAM.</p></center>
          </div>
        </div>

        <p>
          As shown above, DREAM learns to optimally explore (left) by visiting all of the helpful
          (colored) buses, while ignoring all the unhelpful (gray) buses.
          During exploitation episodes (right), this allows it to ride the bus that
          leads closest to the goal, which yields optimal returns.
        </p>

        <p>
          In contrast, as shown below, both IMPORT and E-RL<sup>2</sup> get
          stuck in a local optimum due to the chicken-and-egg problem, where
          they learn not to explore at all (left), opting to immediately end the
          exploration episode.
          This yields suboptimal exploitation behavior (right), which walks to
          the goal, since the bus destinations are not learned during exploration.
        </p>

        <div class="table-like" style="justify-content:space-evenly;max-width:1100px;margin:auto;padding:0px">
          <div>
            <a href="resources/city/rl2_exploration.gif"><img src="resources/city/rl2_exploration.gif" width="375px"></img></a>
            <center><p align="center"><b>Exploration</b> learned by E-RL<sup>2</sup> and IMPORT.</p></center>
          </div>

          <div>
            <a href="resources/city/rl2_exploitation.gif"><img src="resources/city/rl2_exploitation.gif" width="375px"></img></a>
            <center><p align="center"><b>Exploitation</b> learned by E-RL<sup>2</sup> and IMPORT.</p></center>
          </div>
        </div>

        <hr>
        <center><h2>Cooking</h2></center>

        <center>
          <a href="resources/city_examples.svg"><img src="resources/cooking_example.svg" width="800px"></img></a>
          <p align="center" style="width:600px">
            Examples of different problems and goals.
            In different <i>problems</i>, the three fridges on the right are
            different colors, indicating different ingredients.
            In different <i>episodes</i>, the goal (recipe) changes to be
            placing two ingredients in the pot in the correct order.
          </p>
        </center>

        <p>
          In the <i>cooking</i> task, the agent must place two ingredients in
          the pot in the order specified by the recipe (goal).
          The (color-coded) ingredients in the fridges change in different problems.
          See the examples above.
          The agent receives positive reward for each step of:
          1) picking up the correct first ingredient;
          2) placing it in the pot;
          3) picking up the correct second ingredient;
          4) placing it in the pot.
          The agent receives a per timestep negative reward and also a large
          negative reward for picking up the wrong ingredient (as it goes to
          waste).
        </p>

        <div class="table-like" style="justify-content:space-evenly;max-width:1100px;margin:auto;padding:0px">
          <div>
            <a href="resources/cooking/dream_exploration.gif"><img src="resources/cooking/dream_exploration.gif" width="300px" style="padding:0px"></img></a>
            <center><p align="center"><b>Exploration</b> learned by DREAM.</p></center>
          </div>

          <div>
            <a href="resources/cooking/dream_exploitation.gif"><img src="resources/cooking/dream_exploitation.gif" width="300px"></img></a>
            <center><p align="center"><b>Exploitation</b> learned by DREAM.</p></center>
          </div>
        </div>

        <p>
          As shown above, DREAM learns to optimally explore by picking up the
          ingredients from each of the fridges one-by-one, which allows it to
          learn where each ingredient is located (left).
          Then during exploitation episodes (right), it then achieve optimal
          returns by directly retrieving the ingredients specified by the recipe.
        </p>

        <p>
          In contrast, as shown below, E-RL<sup>2</sup> and IMPORT, get stuck in a local
          optimum due to the chicken-and-egg problem, where they suboptimally
          explore by only visiting a single fridge (left).
          Consequently, during exploitation episodes (right), they don't know
          where the purple ingredient is located, so they wastefully
          accidentally grab an extra green ingredient from the top-right fridge,
          which incurs high negative reward.
        </p>

        <div class="table-like" style="justify-content:space-evenly;max-width:1100px;margin:auto;padding:0px">
          <div>
            <a href="resources/city/rl2_exploration.gif"><img src="resources/cooking/rl2_exploration.gif" width="300px" style="padding:0px"></img></a>
            <center><p align="center"><b>Exploration</b> learned by E-RL<sup>2</sup> and IMPORT.</p></center>
          </div>

          <div>
            <a href="resources/city/rl2_exploitation.gif"><img src="resources/cooking/rl2_exploitation.gif" width="300px"></img></a>
            <center><p align="center"><b>Exploitation</b> learned by E-RL<sup>2</sup> and IMPORT.</p></center>
          </div>
        </div>

        <hr>
        <center><h2>Sparse-Reward 3D Visual Navigation</h2></center>

        <div class="table-like" style="justify-content:space-evenly;max-width:1100px;margin:auto;padding:0px">
          <div>
            <a href="resources/green.png"><img src="resources/green.png" width="375px"></img></a>
            <center><p align="center">Sign reads <b style="color:green;">green</b>.</p></center>
          </div>

          <div>
            <a href="resources/red.png"><img src="resources/red.png" width="375px"></img></a>
            <center><p align="center">Sign reads <b style="color:red;">red</b>.</p></center>
          </div>
        </div>

        <p>
          The sparse-reward 3D visual navigation task evalutes agents' ability to
          scale to problems with high-dimensional visual observations and
          sophisticated exploration challenges, pictured above.
          We've made the benchmark from Kamienny et al., 2020 harder by
          including a visual sign and more objects.
          Each episode contains a goal to collect either a block or key.
          The agent starts episodes on the far side of the barrier, and must
          walk around the barrier to read the sign
          (highlighted in <b style="color:#e6e600;">yellow</b>),
          which in the three versions of the problem, specify going to the
          <b style="color:green;">green</b>, <b style="color:red;">red</b>,
          or <b style="color:blue;">blue</b> (not shown)
          version of the object.
          The agent receives 80x60 RGB images as observations and can turn
          left or right, or move forward.
          Going to the correct (correct color and shape)
          object gives reward <b>+1</b> and going to the
          wrong object gives reward <b>-1</b>.
          Otherwise, the agent receives <b>0</b> reward.
        </p>

        <p>
          DREAM learns near-optimal exploration and exploitation behaviors on this
          task, which are pictured below. On the left, DREAM spends the
          exploration episode walking around the barrier to read the sign,
          which says <b style="color:blue;">blue</b>.
          On the right, during an exploitation episode, DREAM receives the
          goal to collect the key. Since DREAM already read that the sign said
          <b style="color:blue;">blue</b> during the exploration episode, it collects the
          <b style="color:blue;">blue</b> key.
        </p>

        <div class="table-like" style="justify-content:space-evenly;max-width:1100px;margin:auto;padding:0px">
          <div>
            <a href="resources/explore.gif"><img src="resources/explore.gif" width="375px" style="padding:0px"></img></a>
            <center><p align="center"><b>Exploration</b> learned by DREAM.</p></center>
          </div>

          <div>
            <a href="resources/execute.gif"><img src="resources/execute.gif" width="375px"></img></a>
            <center><p align="center"><b>Exploitation</b> learned by DREAM (collect the key).</p></center>
          </div>
        </div>

        <p>
          <b>Comparisons.</b>
          Broadly, prior meta-RL approaches fall into two main groups:
          (i) <i>end-to-end</i> approaches, where exploration and exploitation are
          optimized end-to-end based on exploitation rewards, and
          (ii) <i>decoupled</i> approaches, where exploration and exploitation are
          optimized with separate objectives.
          We compare DREAM with state-of-the-art approaches from both
          categories.  In the end-to-end category, we compare with:

          <ul>
            <li>
              E-RL<sup>2</sup> (Stadie et al., 2018), the
              canonical end-to-end approach, which learns a recurrent policy
              conditioned on the entire sequence of past state and reward
              observations, modified to better learn exploration.
            </li>
            <li>
              VariBAD (Zintgraf et al., 2019), which additionally adds
              auxiliary loss functions to the hidden state of the recurrent
              policy to predict the rewards and dynamics of the current
              problem.
              This can be viewed as learning the <i>belief state</i>
              (Kaelbling et al., 1998), a sufficient summary of all of its past
              observations.
            </li>
            <li>
              IMPORT (Kamienny et al., 2020), which additionally leverages the
              problem identifier to help learn exploitation behaviors.
            </li>
          </ul>

          Additionally, in the decoupled category, we compare with:
          <ul>
            <li>
              PEARL-UB, an upperbound on PEARL (Rakelly et al., 2016). We
              analytically compute the expected rewards achieved by the optimal
              problem-specific policy that explores with Thompson sampling
              (Thompson, 1933) using the true posterior distribution over
              problems.
            </li>
          </ul>
        </p>

        <p>
          <b>Quantitative results.</b>
          Below, we plot the returns achieved by all approaches.
          In contrast to DREAM, which achieves near-optimal returns,
          we find that the end-to-end approaches get stuck in local optima,
          indicative of the coupling problem.
          They never read the sign, and
          consequently avoid all objects, in fear of receiving negative reward
          for going to the wrong object.
          This achieves 0 reward.
        </p>

        <p>
          On the other hand, while existing approaches in the decoupled category
          avoid the coupling problem, optimizing their objectives does not lead
          to the optimal exploration policy.
          For example, Thompson sampling approaches (PEARL-UB) do not achieve
          optimal reward, even with the optimal problem-specific exploitation
          policy and access to the true posterior distribution over problems.
          Recall that Thompson sampling explores by sampling a problem from the
          posterior distribution and following the exploitation policy for that
          problem.
          Since the optimal exploitation policy directly goes to the correct
          object, and never reads the sign, Thompson sampling never reads the
          sign during exploration.
        </p>

        <center>
          <a href="resources/sign.png"><img src="resources/sign.png" width="500px"></img></a>
          <p align="center" style="width:500px">
          Only DREAM reads the sign and solves the task.</p>
        </center>

        <hr>
        <center id="acknowledgements"><h1>Additional Experiments</h1></center>
        <p>
          In the experiments, we evaluated DREAM to answer the following questions:
          <ul>
            <li>
              Can DREAM efficiently explore to discover only the task-relevant information
              in the face of distractions?
            </li>
            <li>
              Can DREAM leverage informative objects to aid exploration?
            </li>
            <li>
              Can DREAM learn exploration and exploitation behaviors that generalize to unseen problems?
            </li>
            <li>
              Can DREAM scale to challenging exploration problems with
              high-dimensional states and sparse rewards?
            </li>
          </ul>
          Broadly, the answer is yes to all of these questions.
          Check out our paper for detailed results!
        </p>

      </br><hr>

      <center><h1>Source Code</h1></center>
      <p>Check out the code for DREAM and the benchmarks we evaluate on at GitHub!</p>
      <div class="table-like">
        <span style="font-size:28px"><a href="https://github.com/ezliu/dream">[GitHub]</a></span>
      </div>
      </br><hr>

      <center><h1>Paper and BibTeX</h1></center>
      <table align=center width=800px>
        <tr>
        <td width=400px align=left>
          <pre align=left><code>@article{liu2020decoupling,
  title={Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices},
  author={Liu, Evan Zheran and Raghunathan, Aditi and Liang, Percy and Finn, Chelsea},
  journal={arXiv preprint arXiv:2008.02790},
  year={2020}
}</code></pre>
        </td>
      </table>
      <div class="table-like">
        <span style="font-size:28px"><a href="https://arxiv.org/abs/2008.02790">[ArXiv]</a></span>
      </div>
    <br><hr>
      <center id="acknowledgements"><h1>Acknowledgements</h1></center>
        <p>
          This website is adapted from
          <a href="https://orybkin.github.io/video-gcp/">this website</a>, which was
          in turn adapted from
          <a href="https://pathak22.github.io/modular-assemblies/">this website</a>.
          Feel free to use this website as a template for your own projects by referencing this!
        </p>

        <p>
          The grid world animations were made by
          <a href="https://www.linkedin.com/in/arkira/">Arkira Chantaratananond</a>.
        </p>

        <p>
          Icons used in some of the above figures were made by Freepik,
          ThoseIcons, dDara, Pixel perfect, ThoseIcons, mynamepong, Icongeek26,
          and Vitaly Gorbachev from
          <a href="https://www.flaticon.com">flaticon.com</a>.
        </p>
    </table>
</div>
</body>
</html>
